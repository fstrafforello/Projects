{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Grab original text from Wikipedia\n",
    "\n",
    "We will first need to create a personal API token by following the tutorial: [Wikimedia API Portal](https://api.wikimedia.org/wiki/Documentation/Getting_started). Noted that the rate limit for personal API token is limited to 5,000 requests per hour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "def get_response(search_query, language_code = 'en', number_of_results = 1):\n",
    "  '''\n",
    "  search query is the topic that we want to search\n",
    "  search_query = 'Obama' # example\n",
    "\n",
    "  language code is telling us what language we want it to search in. \n",
    "  language_code = 'en' # example\n",
    "\n",
    "  number_of_results is the number of url results that we want\n",
    "  number_of_results = 1 # example\n",
    "  '''\n",
    "  headers = {\n",
    "    'Authorization': '',\n",
    "    'User-Agent': 'ChatGPT-Detection-Project'\n",
    "  }\n",
    "\n",
    "  base_url = 'https://api.wikimedia.org/core/v1/wikipedia/'\n",
    "  endpoint = '/search/page'\n",
    "  url = base_url + language_code + endpoint\n",
    "  parameters = {'q': search_query, 'limit': number_of_results}\n",
    "  response = requests.get(url, headers=headers, params=parameters)\n",
    "  return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get article title, description, and URL from the search results\n",
    "import json\n",
    "def get_url(response, language_code):\n",
    "  '''\n",
    "  input: response from get_response function & language code\n",
    "  output: url list\n",
    "  '''\n",
    "  response = json.loads(response.text)\n",
    "  urls = []\n",
    "\n",
    "  for page in response['pages']:\n",
    "    display_title = page['title']\n",
    "    article_url = 'https://' + language_code + '.wikipedia.org/wiki/' + page['key'] # get the url of the wikipedia page\n",
    "    urls.append(article_url)\n",
    "    # try:\n",
    "    #   article_description = page['description']\n",
    "    # except:\n",
    "    #   article_description = 'a Wikipedia article'\n",
    "      \n",
    "  return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\" \n",
    "    remove parentheses in text and also the contents within parentheses\n",
    "    \"\"\"\n",
    "    cleaned = ''\n",
    "    paren = 0\n",
    "    for i in text:\n",
    "        if i == '(':\n",
    "            paren += 1\n",
    "        elif i == ')':\n",
    "            paren -= 1\n",
    "        elif paren ==0 and i != ')':\n",
    "            cleaned += i\n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "def grab_article(urls, limit = 100):\n",
    "    '''\n",
    "    urls: List\n",
    "    urls are grabbed by get_url function\n",
    "\n",
    "    return type can have two individual input: \"one\" or \"multiple\"\n",
    "\n",
    "    when return_type = 'one':\n",
    "    start is the paragraph number that we want\n",
    "    \n",
    "    when return_type = 'multiple'\n",
    "    start is the starting paragraph number\n",
    "    end is the ending paragraph number\n",
    "    '''\n",
    "    articles = []\n",
    "    for article_url in urls:\n",
    "        response = requests.get(article_url)\n",
    "        html_content = response.content\n",
    "\n",
    "        # to bs obj\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "        \n",
    "        body_content = soup.find('div', {'id': 'bodyContent'})\n",
    "        # Extract the paragraphs of the main text\n",
    "        paragraphs = body_content.find_all('p')\n",
    "\n",
    "        # Join the paragraphs together into a single string\n",
    "        main_text = '\\n'.join([p.text for p in paragraphs])\n",
    "\n",
    "        article_text = re.sub(r'\\[.*?\\]', '', main_text)\n",
    "\n",
    "        article = article_text.split('\\n') \n",
    "        # for elem in article:\n",
    "        #     if (len(elem) <300) :\n",
    "        #         article.remove(elem)\n",
    "        if len(article[0]) > limit:\n",
    "            articles.append(clean_text(article[0].strip()))\n",
    "\n",
    "    return articles"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. OpenAI API: paraphrase text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use openai api to rephrase the text\n",
    "import openai\n",
    "\n",
    "# Set up the OpenAI API client\n",
    "openai.api_key = \"Secret\"\n",
    "\n",
    "# Define a function to rephrase a given text\n",
    "def rephrase_text(text):\n",
    "    '''\n",
    "    input: human generated text\n",
    "    output: AI rephrased text\n",
    "    '''\n",
    "    response = openai.Completion.create(\n",
    "        engine=\"text-davinci-003\",\n",
    "        prompt=(f\"Rephrase the first section from:\\n{text}\"),\n",
    "        temperature = 1.3,\n",
    "        max_tokens=2048,\n",
    "        \n",
    "        stop=None,\n",
    "        timeout=15,\n",
    "    )\n",
    "\n",
    "    #Extract text\n",
    "    rephrased_text = response.choices[0].text\n",
    "\n",
    "    return rephrased_text.strip(\"\\n\") # get rid off \\n at the beginning of text\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Main code to collect the data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Important notice:\n",
    "\n",
    "Due to the character limit we set in `grab_article`, the rate of sucessfully filtered text is slightly low. Therefore, it is recommended to increase `number_of_results` in the `get_both_text`.\n",
    "\n",
    "ex: \n",
    "search queries = ['television', 'game'], language_code = 'en', number_of_results = 20 (20 searchs for each topic in search queries, so 40 in total)\n",
    "\n",
    "limit = 100\n",
    "\n",
    "number of text after filtering is 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "def get_both_text(search_queries, language_code = 'en', number_of_results = 1):\n",
    "    '''\n",
    "    this function used all the previous function and give you both AI generated text and human generated text\n",
    "\n",
    "    function get_response:\n",
    "\n",
    "        search queries is list of the topics that we want to search\n",
    "        search_queries = ['Obama'] # example\n",
    "\n",
    "        language code is telling us what language we want it to search in. \n",
    "        language_code = 'en' # example\n",
    "\n",
    "        number_of_results is the number of url results that we want for each topic in search query\n",
    "        number_of_results = 1 # example and this will only give 1 url for each element in search queries\n",
    "\n",
    "    function grab_article: \n",
    "\n",
    "        urls is list of wikipedia urls we obtained and want to grab the first paragraph\n",
    "\n",
    "        limit is number of minimum character the text must have.\n",
    "        limit = 100 # by default\n",
    "\n",
    "    return format: Human_Generated, AI_generated\n",
    "    '''\n",
    "    \n",
    "    ai_generated_text = []\n",
    "    human_generated_text = []\n",
    "\n",
    "    for query in search_queries:\n",
    "        print(f'topic is: {query}')\n",
    "        response = get_response(query,language_code = language_code, number_of_results = number_of_results) # get the response from wikipedia\n",
    "        url_list = get_url(response,language_code = language_code) # get all the urls \n",
    "        print(\"urls: \\n\", url_list)\n",
    "        articles = grab_article(url_list, limit = 100) # get all the article with the relative paragraph number, now everything running with default\n",
    "        human_generated_text.extend(articles)\n",
    "        for i in articles:\n",
    "            rephrased_text = rephrase_text(i)\n",
    "            time.sleep(5)\n",
    "            ai_generated_text.append(rephrased_text)\n",
    "\n",
    "    # pd.DataFrame({'Human': [article], 'AI': [rephrased_text]}) # if you want it in a dataframe, you can start with editing this code :)\n",
    "    \n",
    "    return human_generated_text, ai_generated_text # return human_generated, ai_generated\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_lsts = ['Air sports','American football','Association football','Auto racing','Baseball','Basketball','Boating','Boxing','Canoeing','Cricket','Cycling','Exercise','Fishing','Golf','Gymnastics','Hobbies','Horse racing','Ice hockey','Lacrosse','Olympic Games','Rugby league','Rugby union','Sailing','Skiing','Swimming','Tennis','Track and field','Walking trails','Water sports','Whitewater sports']\n",
    "print(f\"There are in total {len(topic_lsts)} topics\")\n",
    "human_generated, ai_generated = get_both_text(topic_lsts,language_code = 'en', number_of_results = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# human_generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ai_generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(human_generated)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Write it to tsv file\n",
    "\n",
    "path : main_page/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "path = \"../data/parallel_text_3.tsv\"\n",
    "\n",
    "with open(path, 'w+') as write_tsv:\n",
    "    writer = csv.writer(write_tsv, delimiter='\\t') \n",
    "    writer.writerow(['human_text', 'ai_text'])\n",
    "    for h, a in zip(human_generated, ai_generated):\n",
    "        writer.writerow([h,a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(path, sep='\\t')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['human_text'][733]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ai_text'][733]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
